{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Image Generation from Text **\n",
    "* Converting text or high-level language (English sentences) to realistic images, ie depicting the content of the text in a picture, would be a very influential aspect in fields like training artificial intelligence . For example, a statement such as “ this girl is wearing black dress and is carrying a white purse” should give us an image of a girl in a black dress with a white purse. Another example, taken from a research paper on similar context, [reference: Generative Adversarial Text-to-Image Synthesis http://arxiv.org/abs/1605.05396]. \n",
    "* Few examples:\n",
    "\n",
    "** The following image is generated by training on the CUB dataset of birds. **\n",
    "\n",
    "<img src='birds1.PNG'>\n",
    "    \n",
    "    \n",
    "** The following image is generated from the oxford flowers dataset. **\n",
    "\n",
    "<img src='flowers.PNG'>\n",
    "\n",
    "** Both the images above prominently had a single object. The follwing image generated based on the Microsoft COCO dataset, has multiple objects in the frame. **\n",
    "\n",
    "<img src='coco.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations.Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room\n",
    "interiors. Realistic image generation from text description could have major impact on feilds such as interactive computational graphic design, training and tuning Artifical Intelligence etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this project, I will be using Deep Convolutional GANs (DCGANs). They are an improvement over vanilla GANs which were first developed and are more stable in terms of training and generate higher quality samples. The following changes were made for DCGANS\n",
    "\n",
    "* Batch normalization is a must in both networks.\n",
    "\n",
    "* ReLU activations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** System Architechture **\n",
    "<img src='sys1.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### GAN for text to image conversion (StackGan)\n",
    "\n",
    "** Step 1 : Preprocess the IMAGES AND CAPTIONS **\n",
    "\n",
    "* In image preprocessing, the images are resized to unify the image dimensions.\n",
    " The data is divided into training and testing set.\n",
    " The images data is then converted to pickle files for training the generative model.\n",
    "\n",
    "* The captions data is extracted and a tensorlayer vocabulary is built. The vocab list and the captions data are then pickled and saved for training the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generative Adversarial networks\n",
    "** Generative adversarial networks (GANs)** are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. They were introduced by Ian Goodfellow et al. in 2014.\n",
    "\n",
    "GANS have been the most efficient in generation of content and have been modified and improved since they were developed for the first time.\n",
    "\n",
    "GAN's evolution:(types of GANs)\n",
    "\n",
    "* DCGANS\n",
    "* Improved DCGANS\n",
    "* Condtional GANs\n",
    "* Info GANs\n",
    "* Wasserstein GANs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GAN Architechture\n",
    "\n",
    "\n",
    "GANs are composed of two networks,\n",
    "* Generator network :  A deep network generates realistic images.\n",
    "* Discriminator network: A deep network distinguishes real images from computer generated images.\n",
    "\n",
    "* Both networks are trained at the same time and compete against each other in a minmax game.\n",
    "\n",
    "<img src='Ganoverview.PNG'>\n",
    "\n",
    "* Step 1: Generator generates images by sampling a vector noise Z from a simple distribution (e.g. normal) and then upsampling this vector up to an image. Initially gnerated images look very noisy.\n",
    "\n",
    "* Step 2: The discriminator is given fake and real images and learns to distinguish them. \n",
    "\n",
    "* Step 3: The generator later receives the “feedback” of the discriminator through a backpropagation step, becoming better at generating images.\n",
    "\n",
    "* Result: The distribution of fake images should be as close as possible to the distribution of real images, ie the generated images should look as real as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN (Deep Convolutional Generative Adversarial Networks)\n",
    "* First improvement over the vanilla GAN\n",
    "* Added Batch normalization and ReLU activation on both the networks\n",
    "\n",
    "An image directly generated using a deep network while using a second discriminator network to guide the generation process.\n",
    "\n",
    "This method trains a deep convolutional generative adversarial network (DC-GAN) conditioned on text features encoded by a hybrid character-level convolutional recurrent neural network. Both the generator network G\n",
    "and the discriminator network D perform feed-forward inference conditioned on the text feature.\n",
    "\n",
    "* ** Generator network : G : R<sup>Z</sup> x R<sup>T</sup> -> R<sup>D</sup> **\n",
    "* ** Discriminator network : D : R<sup>D</sup> x R<sup>T</sup> -> {0,1} **\n",
    "\n",
    "T: dimension of the textdescription embedding\n",
    "D: dimension of the image,\n",
    "Z: dimension of the noise input to G\n",
    "\n",
    "Step 1 : encode the text query t using text encoder ϕ. The description embedding ϕ(t) is first compressed to a smaller dimension and then concatenated to the noise vector Z.\n",
    "\n",
    "Step 2 : standard deconvolutional network follows and a synthetic image is generated\n",
    "\n",
    "Step 2 : several layers of stride-2 convolution with spatial batch normalization followed by leaky ReLU\n",
    "\n",
    "<img src='dcgan.PNG'>\n",
    "\n",
    "* Steps of the algorithm\n",
    "Input : minibatch images x, matching text t, mismatching,tˆ, number of training batch steps S\n",
    "\n",
    "* 1.encode matching text descriptions\n",
    "* 2.Encode mis-matching text description\n",
    "* 3.Draw sample of random noise\n",
    "* 4.Forward through generator, generate image\n",
    "* 5.calculate score for real image and right text (image and its coresponding text)\n",
    "* 6.calculate score for real image wrong text\n",
    "* 7.calculate score for fake image, right text\n",
    "* 8.Determine the scores of discriminator and generators\n",
    "* 9.Update the discriminator/generator by determinig the gradient of D and G 's objective with respect to its parameters.\n",
    "\n",
    "* Batch normalization is used in both the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacked Generative Adversarial Networks\n",
    "\n",
    "* To generate high resolution images \n",
    "\n",
    "\n",
    "<img src='stackgan.PNG'>\n",
    "\n",
    "StackGAN are trained in two stages,\n",
    "\n",
    "* Initially the text description of the image is taken as input and an image of size 64x64 is produced using architecture similar to DCGANs\n",
    "\n",
    "* Then the generated 64x64 image(downsampled to 512*16*16) and the caption of the image are passed as inputs to the StageII generator and it generates the images of size 256x256.\n",
    "\n",
    "* In both the stages, the discriminator is trained on the caption and the corresponding generated image as input.\n",
    "\n",
    "* In StackGan, they first trained the StageI generator, then generated the images of size 64x64 and used those images to generated 256x256 size images from StageII \n",
    "\n",
    "<img src ='stackgan2.PNG'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "    \n",
    "* Test the models on multi-object images on a larger dataset approx, 120,000 training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "\n",
    "https://jhui.github.io/2017/03/05/Generative-adversarial-models/\n",
    "\n",
    "http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them\n",
    "\n",
    "https://github.com/zsdonghao/text-to-image\n",
    "\n",
    "https://github.com/hanzhanggit/StackGAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
